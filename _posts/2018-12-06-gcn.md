---
layout:     post
title:      【Draft】GCN理论和应用
subtitle:   图卷积网络的来龙去脉
date:       2018-12-06
author:     Oscar Zhang
header-style:   text
catalog:    true
mathjax:    true
tags:
    - Coding
    - Python
---

## 核心：图卷积

在计算机视觉领域，Convolutional Neural Network(CNN)已经取得了巨大成就，主要针对Euclidean Structure的数据（图像属此类型）。

![][1]

在Topological Structure（拓扑结构）数据中，相应的工具则是Graph Convolutional Network，可以应对更广义的结构数据。

![][2]

GCN在ICLR 2017由T.Kipf, M.Welling正式提出(Semi-supervised Classification with Graph Convolutional Networks, ICLR 2017)。

![][3]

先介绍一下图的基础知识。对于一个图：

![][10]

对应的数学表示为：

| Degree Matrix $$D$$|  Adjacency Matrix $$A$$ | Laplacian Matrix $$L$$ |
| :-----:   | :----: | :----: |
| $$ \begin{pmatrix} 2 &  &  &  &  & \\  & 3 &  &  &  & \\  &  & 2 &  &  & \\  &  &  & 3 &  & \\  &  &  &  & 3 & \\  &  &  &  &  & 1 \end{pmatrix} $$ |$$\begin{pmatrix} & 1 &  &  & 1 & \\ 1 &  & 1 &  & 1 & \\  & 1 &  & 1 &  & \\  &  & 1 &  & 1 & 1\\ 1 & 1 &  & 1 &  & \\  &  &  & 1 &  & \end{pmatrix}$$| $$\begin{pmatrix}2 & -1 &  &  & -1 & \\ -1 & 3 & -1 &  & -1 & \\  & -1 & 2 & -1 &  & \\  &  & -1 & 3 & -1 & -1\\ -1 & -1 &  & -1 & 3 & \\  &  &  & -1 &  & 1\end{pmatrix}$$|

在GCN文中一开始便给出了核心，每两层直接的递推公式：

$$H^{(l+l)}=\sigma(\tilde{D}^ {-\frac{1}{2}}\tilde{A}\tilde{D}^ {-\frac{1}{2}}H^{(l)}W^{(l)})$$

其中，$$\tilde {A}$$是$$A$$加上单位对角矩阵；$$\tilde {D}$$是相应的度；$$H^{(l)}$$为第$$l$$层以各节点特征为行向量的矩阵（$$H^{(0)}$$为输入）；$$W$$为权重矩阵，每层不同，是训练最终要得到的参数；$$\sigma$$是激活函数。   

## 图卷积的来龙

我看到卷积公式之后，内心是崩溃的：为什么是这样一个公式，有什么道理吗？总不能是试来的吧？      
抱着为什么的疑问，我对其来龙进行了简要的学习，这个过程还是蛮有意思的。下面是一个简要介绍，省略了数学的性质和推导，主要看中心思想。

首先，图的Laplacian matrix $$L$$是半正定对称矩阵，利用其性质，可以进行矩阵分解

$$L=U \begin{pmatrix}\lambda_{1} &  & \\ & \ddots & \\ &  & \lambda_{n}\end{pmatrix} U^{-1}$$

其中，$$\begin{pmatrix}\lambda_{1} &  & \\ & \ddots & \\ &  & \lambda_{n}\end{pmatrix}$$是$$L$$的$$n$$个特征值组成的对角阵，$$U=(\overrightarrow{u_{1}},\overrightarrow{u_{2}},\dots{},\overrightarrow{u_{n}})$$是列向量为单位特征向量的矩阵。

因为$$U$$是正交矩阵，有$$UU^{T}=E$$,所以有：

$$L=U \begin{pmatrix}\lambda_{1} &  & \\ & \ddots & \\ &  & \lambda_{n}\end{pmatrix} U^{T}$$

下面，就是如何利用包含图结构的$$L$$，和代表其特性的的$$\begin{pmatrix}\lambda_{1} &  & \\ & \ddots & \\ &  & \lambda_{n}\end{pmatrix}$$和$$U$$推导出图卷积的过程。

众所周知，卷积是信号处理中的概念，与Fourier transform息息相关。GCN的概念也是从此推广而来。     
故事要从Fourier transform说起。傅立叶正变换：

$$F(\omega)=\mathcal{F}[f(t)]=\int f(t)e^{-i \omega t}\,dt$$

公式很复杂，这里只说它的思想。傅立叶变换用形象一些的图表示为：

![][5]

傅立叶逆变换，就是上图的逆向过程：

$$F(\omega)=\mathcal{F}[f(t)]=\frac {1}{2\pi} \int f(t)e^{i \omega t}\,dt$$     

从物理角度理解傅里叶变换是以一组特殊的函数为正交基，对原函数进行线性变换，物理意义便是原函数在各组基函数的投影。这跟拉普拉斯矩阵的特征值分解有相同的本质。       
如果将$$[f(1), \dots,f(N)]$$和$$[u_{l}(1), \dots,u_{l}(N)]$$看成时间信号，就可以套用傅里叶变换了。     
类推Topological Graph上的傅立叶变换：

$$\hat{f}(\lambda_{l})=\sum_{i=1}^{n}f(i)u_{l}^{*}(i)$$

矩阵形式为

$$\hat{f}=U^{T}f$$         

具体来说，就是     

$$\begin{pmatrix}\hat{f}({\lambda_{1}})\\ \hat{f}({\lambda_{2}})\\\vdots\\ \hat{f}({\lambda_{N}})\end{pmatrix}=\begin{pmatrix}
u_{1}(1) & u_{1}(2) & \dots & u_{1}(N)\\ 
u_{2}(1) & u_{2}(2) & \dots & u_{2}(N)\\ 
\vdots & \vdots & \ddots & \vdots \\ 
u_{N}(1) & u_{N}(2) & \dots & u_{N}(N)
\end{pmatrix}\begin{pmatrix}f(1)\\ f(2)\\ \vdots\\ f(N)\end{pmatrix}$$   
   
其中，$$f(n)$$是拓扑图中第n个节点的特征，$$\hat{f}({\lambda_{n}})$$是在特征值的域上的特征。         
相应的逆变换为：

$$\begin{pmatrix}f(1)\\ f(2)\\ \vdots\\ f(N)\end{pmatrix}=\begin{pmatrix}
u_{1}(1) & u_{2}(1) & \dots & u_{N}(1)\\ 
u_{1}(2) & u_{2}(2) & \dots & u_{N}(2)\\ 
\vdots & \vdots & \ddots & \vdots \\ 
u_{1}(N) & u_{2}(N) & \dots & u_{N}(N)
\end{pmatrix}\begin{pmatrix}\hat{f}({\lambda_{1}})\\ \hat{f}({\lambda_{2}})\\\vdots\\ \hat{f}({\lambda_{N}})\end{pmatrix}$$

卷积推广到图中：

傅里叶卷积：

$$\mathcal{f*h} = \mathcal{F}^{-1}[\hat{\mathcal{f}}(\omega)\hat{\mathcal{h}}(\omega)]=\frac{1}{2\pi}\int \hat{\mathcal{f}}(\omega)\hat{\mathcal{h}}(\omega)e^{i\omega t}\,d\omega$$

推广到图卷积：

$$(\mathcal{f*h})_{G}(i)=\sum_{l=1}^{N} \hat{\mathcal{f}}({\lambda_{l}})\hat{\mathcal{h}}({\lambda_{l}})u_{l}(i)$$

矩阵形式

$$(\mathcal{f*h})_{G}=U \begin{pmatrix}\hat{h}(\lambda_{1}) &  & \\ & \ddots & \\ &  & \hat{h}(\lambda_{n})\end{pmatrix} U^{T}f$$

最初的做法是将核函数$$\hat{h}(\lambda_{n})$$暴力替换为$$\theta_{n}$$

$$output = \sigma \begin{pmatrix}
U \begin{pmatrix}\theta_{1} &  & \\ & \ddots & \\ &  & \theta_{n}\end{pmatrix} U^{T}f
\end{pmatrix}$$

后来做了一个巧妙的变化

$$output = \sigma \begin{pmatrix}
U \begin{pmatrix}\sum_{j=0}^{K-1}\alpha_{j}\lambda_{1}^{j} &  & \\ & \ddots & \\ &  & \sum_{j=0}^{K-1}\alpha_{j}\lambda_{n}^{j}\end{pmatrix} U^{T}f
\end{pmatrix}$$

令$$\begin{pmatrix}\sum_{j=0}^{K-1}\alpha_{j}\lambda_{1}^{j} &  & \\ & \ddots & \\ &  & \sum_{j=0}^{K-1}\alpha_{j}\lambda_{n}^{j}\end{pmatrix}=\sum_{j=0}^{K-1}\alpha_{j}\Lambda^{j}$$

则有

$$U\sum_{j=0}^{K-1}\alpha_{j}\Lambda^{j}U^{T}=\sum_{j=0}^{K-1}\alpha_{j}U\Lambda^{j}U^{T}=\sum_{j=0}^{K-1}\alpha_{j}U\Lambda^{j-1}U^{T}U\Lambda^{1}U^{T}=\sum_{j=0}^{K-1}\alpha_{j}U\Lambda^{j-1}U^{T}U\begin{pmatrix}\lambda_{1} &  & \\ & \ddots & \\ &  & \lambda_{n}\end{pmatrix}U^{T}=\sum_{j=0}^{K-1}\alpha_{j}U\Lambda^{j-1}U^{T}L=\dots=\sum_{j=0}^{K-1}\alpha_{j}L^{j}$$

最终得到

$$output = \sigma\begin{pmatrix}\sum_{j=0}^{K-1}\alpha_{j}L^{j}f\end{pmatrix}$$

相较于直接替换的好处有：

- 计算复杂度降低
- 卷积核参数减少
- 具有Spatial Localization


[1]: https://raw.githubusercontent.com/zbhoscar/zbhoscar.github.io/master/img/in-post/post-gcn/1.jpg
[2]: https://raw.githubusercontent.com/zbhoscar/zbhoscar.github.io/master/img/in-post/post-gcn/2.jpg
[3]: https://raw.githubusercontent.com/zbhoscar/zbhoscar.github.io/master/img/in-post/post-gcn/3.jpg
[4]: https://raw.githubusercontent.com/zbhoscar/zbhoscar.github.io/master/img/in-post/post-gcn/4.jpg
[5]: https://raw.githubusercontent.com/zbhoscar/zbhoscar.github.io/master/img/in-post/post-gcn/5.jpg
[6]: https://raw.githubusercontent.com/zbhoscar/zbhoscar.github.io/master/img/in-post/post-gcn/6.jpg
[7]: https://raw.githubusercontent.com/zbhoscar/zbhoscar.github.io/master/img/in-post/post-gcn/7.jpg
[8]: https://raw.githubusercontent.com/zbhoscar/zbhoscar.github.io/master/img/in-post/post-gcn/8.jpg
[9]: https://raw.githubusercontent.com/zbhoscar/zbhoscar.github.io/master/img/in-post/post-gcn/9.jpg
[10]: https://raw.githubusercontent.com/zbhoscar/zbhoscar.github.io/master/img/in-post/post-gcn/10.jpg
